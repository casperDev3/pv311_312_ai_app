"""
–†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±'—î–∫—Ç—ñ–≤ –Ω–∞ –≤—ñ–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ –∑ –≤–∏–¥—ñ–ª–µ–Ω–Ω—è–º —Ä–∞–º–∫–æ—é
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤–µ–±-–∫–∞–º–µ—Ä—É, YOLOv5 –¥–ª—è –¥–µ—Ç–µ–∫—Ü—ñ—ó —Ç–∞ ResNet50 –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
"""

import torch
import torchvision.transforms as transforms
import torchvision.models as models
import cv2
import requests
import time
import numpy as np

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ YOLOv5 –¥–ª—è –¥–µ—Ç–µ–∫—Ü—ñ—ó –æ–±'—î–∫—Ç—ñ–≤
print("üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ YOLOv5...")
yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
yolo_model.eval()

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ ResNet50 –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
print("üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ ResNet50...")
model = models.resnet50(pretrained=True)
model.eval()

# –ü–µ—Ä–µ–º—ñ—â—É—î–º–æ –º–æ–¥–µ–ª—ñ –Ω–∞ GPU —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–æ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
yolo_model = yolo_model.to(device)
print(f"‚úÖ –ú–æ–¥–µ–ª—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ –Ω–∞: {device}")

# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º—ñ—Ç–∫–∏ –∫–ª–∞—Å—ñ–≤ ImageNet
LABELS_URL = "https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json"
labels = requests.get(LABELS_URL).json()

# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ–π –¥–ª—è –∫–∞–¥—Ä—ñ–≤ –≤—ñ–¥–µ–æ
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])


def predict_frame(frame):
    """
    –†–æ–∑–ø—ñ–∑–Ω–∞—î –æ–±'—î–∫—Ç –Ω–∞ –æ–¥–Ω–æ–º—É –∫–∞–¥—Ä—ñ –≤—ñ–¥–µ–æ

    Args:
        frame: –∫–∞–¥—Ä –∑ –≤—ñ–¥–µ–æ (numpy array)

    Returns:
        top_class: –Ω–∞–∑–≤–∞ –∫–ª–∞—Å—É –∑ –Ω–∞–π–≤–∏—â–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é
        confidence: –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å —É –≤—ñ–¥—Å–æ—Ç–∫–∞—Ö
    """
    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ BGR (OpenCV) –≤ RGB
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó
    img_tensor = transform(frame_rgb).unsqueeze(0).to(device)

    # –†–æ–±–∏–º–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
    with torch.no_grad():
        outputs = model(img_tensor)

    # –û—Ç—Ä–∏–º—É—î–º–æ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)

    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –∫–ª–∞—Å –∑ –Ω–∞–π–≤–∏—â–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é
    top_prob, top_idx = torch.max(probabilities, 0)

    top_class = labels[top_idx.item()]
    confidence = top_prob.item() * 100

    return top_class, confidence


def draw_text_with_background(frame, text, position, font_scale=0.8, thickness=2):
    """
    –ú–∞–ª—é—î —Ç–µ–∫—Å—Ç –∑ –Ω–∞–ø—ñ–≤–ø—Ä–æ–∑–æ—Ä–∏–º —Ñ–æ–Ω–æ–º –¥–ª—è –∫—Ä–∞—â–æ—ó —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ
    """
    font = cv2.FONT_HERSHEY_SIMPLEX

    # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–æ–∑–º—ñ—Ä —Ç–µ–∫—Å—Ç—É
    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)

    x, y = position
    # –ú–∞–ª—é—î–º–æ –Ω–∞–ø—ñ–≤–ø—Ä–æ–∑–æ—Ä–∏–π –ø—Ä—è–º–æ–∫—É—Ç–Ω–∏–∫
    overlay = frame.copy()
    cv2.rectangle(overlay, (x - 10, y - text_height - 15),
                  (x + text_width + 10, y + 5), (0, 0, 0), -1)
    cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)

    # –ú–∞–ª—é—î–º–æ —Ç–µ–∫—Å—Ç
    cv2.putText(frame, text, (x, y - 5), font, font_scale, (0, 255, 0), thickness)


def draw_text_with_background(frame, text, position, font_scale=0.8, thickness=2):
    """
    –ú–∞–ª—é—î —Ç–µ–∫—Å—Ç –∑ –Ω–∞–ø—ñ–≤–ø—Ä–æ–∑–æ—Ä–∏–º —Ñ–æ–Ω–æ–º –¥–ª—è –∫—Ä–∞—â–æ—ó —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ
    """
    font = cv2.FONT_HERSHEY_SIMPLEX

    # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–æ–∑–º—ñ—Ä —Ç–µ–∫—Å—Ç—É
    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)

    x, y = position
    # –ú–∞–ª—é—î–º–æ –Ω–∞–ø—ñ–≤–ø—Ä–æ–∑–æ—Ä–∏–π –ø—Ä—è–º–æ–∫—É—Ç–Ω–∏–∫
    overlay = frame.copy()
    cv2.rectangle(overlay, (x - 10, y - text_height - 15),
                  (x + text_width + 10, y + 5), (0, 0, 0), -1)
    cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)

    # –ú–∞–ª—é—î–º–æ —Ç–µ–∫—Å—Ç
    cv2.putText(frame, text, (x, y - 5), font, font_scale, (0, 255, 0), thickness)


def run_realtime_detection(camera_id=0, confidence_threshold=0.3):
    """
    –ó–∞–ø—É—Å–∫–∞—î —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±'—î–∫—Ç—ñ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ –∑ –≤–∏–¥—ñ–ª–µ–Ω–Ω—è–º —Ä–∞–º–∫–∞–º–∏

    Args:
        camera_id: ID –∫–∞–º–µ—Ä–∏ (–∑–∞–∑–≤–∏—á–∞–π 0 –¥–ª—è –≤–±—É–¥–æ–≤–∞–Ω–æ—ó –≤–µ–±-–∫–∞–º–µ—Ä–∏)
        confidence_threshold: –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è (0-1)
    """
    # –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ –≤—ñ–¥–µ–æ –ø–æ—Ç—ñ–∫
    cap = cv2.VideoCapture(camera_id)

    if not cap.isOpened():
        print("‚ùå –ü–æ–º–∏–ª–∫–∞: –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –∫–∞–º–µ—Ä—É")
        return

    # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ —Ä–æ–∑–º—ñ—Ä –∫–∞–¥—Ä—É
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    print("\n" + "=" * 60)
    print("üé• –†–û–ó–ü–Ü–ó–ù–ê–í–ê–ù–ù–Ø –û–ë'–Ñ–ö–¢–Ü–í –í –†–ï–ê–õ–¨–ù–û–ú–£ –ß–ê–°–Ü")
    print("=" * 60)
    print("üìå –ù–∞—Ç–∏—Å–Ω—ñ—Ç—å 'q' –¥–ª—è –≤–∏—Ö–æ–¥—É")
    print("üìå –ù–∞—Ç–∏—Å–Ω—ñ—Ç—å 's' –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–∫—Ä—ñ–Ω—à–æ—Ç—É")
    print("=" * 60 + "\n")

    fps_start_time = time.time()
    fps_counter = 0
    fps = 0

    screenshot_counter = 0

    # –ö–æ–ª—å–æ—Ä–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤ –æ–±'—î–∫—Ç—ñ–≤
    colors = [
        (0, 255, 0),  # –ó–µ–ª–µ–Ω–∏–π
        (255, 0, 0),  # –°–∏–Ω—ñ–π
        (0, 0, 255),  # –ß–µ—Ä–≤–æ–Ω–∏–π
        (255, 255, 0),  # –ë–ª–∞–∫–∏—Ç–Ω–∏–π
        (255, 0, 255),  # –ü—É—Ä–ø—É—Ä–Ω–∏–π
        (0, 255, 255),  # –ñ–æ–≤—Ç–∏–π
    ]

    while True:
        ret, frame = cap.read()

        if not ret:
            print("‚ùå –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –∫–∞–¥—Ä—É")
            break

        # –î–µ—Ç–µ–∫—Ü—ñ—è –æ–±'—î–∫—Ç—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é YOLO
        try:
            results = yolo_model(frame)
            detections = results.pandas().xyxy[0]  # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É —Ñ–æ—Ä–º–∞—Ç—ñ pandas

            # –û–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–µ–Ω –≤–∏—è–≤–ª–µ–Ω–∏–π –æ–±'—î–∫—Ç
            for idx, detection in detections.iterrows():
                confidence = detection['confidence']

                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –æ–±'—î–∫—Ç–∏ –∑ –Ω–∏–∑—å–∫–æ—é –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—é
                if confidence < confidence_threshold:
                    continue

                # –û—Ç—Ä–∏–º—É—î–º–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ —Ä–∞–º–∫–∏
                x1, y1, x2, y2 = int(detection['xmin']), int(detection['ymin']), \
                    int(detection['xmax']), int(detection['ymax'])

                # –û—Ç—Ä–∏–º—É—î–º–æ –Ω–∞–∑–≤—É –∫–ª–∞—Å—É
                class_name = detection['name']

                # –í–∏–±–∏—Ä–∞—î–º–æ –∫–æ–ª—ñ—Ä –¥–ª—è —Ä–∞–º–∫–∏
                color = colors[idx % len(colors)]

                # –ú–∞–ª—é—î–º–æ —Ä–∞–º–∫—É –Ω–∞–≤–∫–æ–ª–æ –æ–±'—î–∫—Ç–∞
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

                # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç—É –∑ –Ω–∞–∑–≤–æ—é —Ç–∞ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—é
                label = f"{class_name}: {confidence:.2f}"

                # –ú–∞–ª—é—î–º–æ —Ñ–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç—É
                (text_width, text_height), baseline = cv2.getTextSize(
                    label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2
                )

                # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç—É
                cv2.rectangle(frame, (x1, y1 - text_height - 10),
                              (x1 + text_width + 5, y1), color, -1)

                # –¢–µ–∫—Å—Ç
                cv2.putText(frame, label, (x1 + 2, y1 - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –¥–µ—Ç–µ–∫—Ü—ñ—ó: {e}")

        # –û–±—á–∏—Å–ª—é—î–º–æ FPS
        fps_counter += 1
        if time.time() - fps_start_time >= 1.0:
            fps = fps_counter
            fps_counter = 0
            fps_start_time = time.time()

        # –í—ñ–¥–æ–±—Ä–∞–∂–∞—î–º–æ FPS –∑ —Ñ–æ–Ω–æ–º
        fps_text = f"FPS: {fps}"
        (text_width, text_height), _ = cv2.getTextSize(
            fps_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
        )
        cv2.rectangle(frame, (5, frame.shape[0] - 30),
                      (15 + text_width, frame.shape[0] - 10), (0, 0, 0), -1)
        cv2.putText(frame, fps_text, (10, frame.shape[0] - 15),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

        # –í—ñ–¥–æ–±—Ä–∞–∂–∞—î–º–æ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
        cv2.putText(frame, "Natysni 'q' - vyhid, 's' - screenshot",
                    (10, 25), cv2.FONT_HERSHEY_SIMPLEX,
                    0.5, (255, 255, 255), 1)

        # –ü–æ–∫–∞–∑—É—î–º–æ –∫–∞–¥—Ä
        cv2.imshow('Rozpiznavannya ob\'yektiv z vydilennyam', frame)

        # –û–±—Ä–æ–±–∫–∞ –∫–ª–∞–≤—ñ—à
        key = cv2.waitKey(1) & 0xFF

        if key == ord('q'):
            print("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏...")
            break
        elif key == ord('s'):
            screenshot_counter += 1
            filename = f"screenshot_{screenshot_counter}.jpg"
            cv2.imwrite(filename, frame)
            print(f"üì∏ –°–∫—Ä—ñ–Ω—à–æ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {filename}")

    # –ó–≤—ñ–ª—å–Ω—è—î–º–æ —Ä–µ—Å—É—Ä—Å–∏
    cap.release()
    cv2.destroyAllWindows()
    print("‚úÖ –†–æ–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


# ============================================
# –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ò
# ============================================

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("  –†–û–ó–ü–Ü–ó–ù–ê–í–ê–ù–ù–Ø –û–ë'–Ñ–ö–¢–Ü–í –ó –í–ï–ë-–ö–ê–ú–ï–†–ò –ó –í–ò–î–Ü–õ–ï–ù–ù–Ø–ú")
    print("=" * 60)

    try:
        # –ó–∞–ø—É—Å–∫ –∑ –≤–µ–±-–∫–∞–º–µ—Ä–æ—é
        run_realtime_detection(camera_id=4, confidence_threshold=0.3)

    except KeyboardInterrupt:
        print("\n\nüëã –ü—Ä–æ–≥—Ä–∞–º–∞ –ø–µ—Ä–µ—Ä–≤–∞–Ω–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
